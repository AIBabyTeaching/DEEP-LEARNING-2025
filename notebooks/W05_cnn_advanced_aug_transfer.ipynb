{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f378c6d",
   "metadata": {},
   "source": [
    "# W05 · Advanced CNNs, Data Augmentation & Transfer Learning\n",
    "\n",
    "This notebook walks through an end-to-end computer vision workflow using the Kaggle **Dogs vs. Cats** dataset.You will download the data programmatically, prepare efficient `tf.data` pipelines with normalization and augmentation,train a VGG16-based classifier for feature extraction, and progressively fine-tune the network for higher accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e2edc",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "* Authenticate with Kaggle and automate dataset downloads directly from a notebook.\n",
    "* Structure image datasets for TensorFlow pipelines and apply normalization plus on-the-fly augmentation.\n",
    "* Leverage `notebooks.dl_utils` utilities to build performant training `tf.data` inputs.\n",
    "* Use a pre-trained VGG16 network as a fixed feature extractor and train a custom classifier head.\n",
    "* Experiment with multiple fine-tuning strategies to adapt the convolutional base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba187af",
   "metadata": {},
   "source": [
    "> **References**  > * `Training a convnet from scratch on a small dataset.ipynb` — baseline augmentation ideas.  > * `2- pretrained convnet.ipynb` — feature extraction vs. fine-tuning comparisons.  > * `dataset_api_kaggle.ipynb` — Kaggle authentication and API usage patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3d198",
   "metadata": {},
   "source": [
    "## Environment & dataset setup\n",
    "\n",
    "Follow the steps below before running any training code:\n",
    "\n",
    "1. Install the Kaggle CLI (once per environment).\n",
    "2. Copy the provided `kaggle.json` credentials into `~/.kaggle/` with restricted permissions.\n",
    "3. Use the new `dl_utils.data.download_kaggle_competition` helper to fetch and extract the **Dogs vs. Cats** files.\n",
    "4. Inspect the extracted archives and build TensorFlow datasets from the raw images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2171107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kaggle if it's not already present in the runtime\n",
    "%pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198de5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from notebooks.dl_utils import data as data_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory layout\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\" / \"dogs-vs-cats\"\n",
    "ARCHIVE_DIR = DATA_ROOT / \"archives\"\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "\n",
    "# Training configuration\n",
    "SEED = 1337\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15  # remaining 0.15 will be used as a hold-out test set\n",
    "SHUFFLE_BUFFER = 2048\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4703960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Kaggle credentials\n",
    "kaggle_config_src = PROJECT_ROOT / \"notebooks\" / \"kaggle.json\"\n",
    "kaggle_config_dst = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "\n",
    "if not kaggle_config_src.exists():\n",
    "    raise FileNotFoundError(\"kaggle.json not found inside the notebooks/ directory\")\n",
    "\n",
    "kaggle_config_dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copy2(kaggle_config_src, kaggle_config_dst)\n",
    "os.chmod(kaggle_config_dst, 0o600)\n",
    "\n",
    "print(f\"Kaggle credentials copied to {kaggle_config_dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the Kaggle Dogs vs. Cats competition files\n",
    "COMPETITION = \"dogs-vs-cats\"\n",
    "archives_path = data_utils.download_kaggle_competition(\n",
    "    COMPETITION,\n",
    "    target_dir=ARCHIVE_DIR,\n",
    "    unzip=True,\n",
    ")\n",
    "\n",
    "print(f\"Competition files downloaded to: {archives_path}\")\n",
    "print(\"Available archives:\")\n",
    "for item in sorted(archives_path.iterdir()):\n",
    "    print(\" •\", item.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50389729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract nested archives (train.zip and test1.zip)\n",
    "TRAIN_ARCHIVE = ARCHIVE_DIR / \"train.zip\"\n",
    "TEST_ARCHIVE = ARCHIVE_DIR / \"test1.zip\"\n",
    "TRAIN_EXTRACT_DIR = RAW_DIR / \"train\"\n",
    "TEST_EXTRACT_DIR = RAW_DIR / \"test1\"\n",
    "\n",
    "for archive, target in [\n",
    "    (TRAIN_ARCHIVE, TRAIN_EXTRACT_DIR),\n",
    "    (TEST_ARCHIVE, TEST_EXTRACT_DIR),\n",
    "]:\n",
    "    if archive.exists():\n",
    "        target.mkdir(parents=True, exist_ok=True)\n",
    "        if not any(target.iterdir()):\n",
    "            with zipfile.ZipFile(archive) as zf:\n",
    "                zf.extractall(target)\n",
    "            print(f\"Extracted {archive.name} -> {target}\")\n",
    "        else:\n",
    "            print(f\"Skipping extraction for {archive.name}; files already present.\")\n",
    "    else:\n",
    "        print(f\"Archive {archive} is missing. Ensure the Kaggle download succeeded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37787d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images and prepare labels\n",
    "train_image_paths = sorted(TRAIN_EXTRACT_DIR.glob(\"*.jpg\"))\n",
    "print(f\"Total train images: {len(train_image_paths):,}\")\n",
    "\n",
    "labels = np.array([1 if path.name.startswith(\"dog\") else 0 for path in train_image_paths], dtype=np.int32)\n",
    "class_names = [\"cat\", \"dog\"]\n",
    "print({name: int((labels == idx).sum()) for idx, name in enumerate(class_names)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcd30e",
   "metadata": {},
   "source": [
    "## Building a normalized & augmented `tf.data` pipeline\n",
    "\n",
    "We will split the dataset into **70% training**, **15% validation**, and **15% testing**. Images are decoded from disk,\n",
    "resized to 224×224, normalized to `[0, 1]`, and augmented on-the-fly for the training split. The helper\n",
    "`data_utils.prepare_for_training` encapsulates the common caching, shuffling, batching, and prefetching steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split using NumPy for reproducibility\n",
    "rng = np.random.default_rng(SEED)\n",
    "indices = np.arange(len(train_image_paths))\n",
    "rng.shuffle(indices)\n",
    "\n",
    "train_end = int(len(indices) * TRAIN_SPLIT)\n",
    "val_end = int(len(indices) * (TRAIN_SPLIT + VAL_SPLIT))\n",
    "\n",
    "split_indices = {\n",
    "    \"train\": indices[:train_end],\n",
    "    \"val\": indices[train_end:val_end],\n",
    "    \"test\": indices[val_end:],\n",
    "}\n",
    "\n",
    "split_paths = {\n",
    "    split: np.array([str(train_image_paths[i]) for i in split_idx])\n",
    "    for split, split_idx in split_indices.items()\n",
    "}\n",
    "\n",
    "split_labels = {\n",
    "    split: labels[split_indices[split]]\n",
    "    for split in split_indices\n",
    "}\n",
    "\n",
    "for split in split_paths:\n",
    "    print(f\"{split.title():<5}: {len(split_paths[split]):,} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-level preprocessing functions\n",
    "def load_and_resize(path: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE, antialias=True)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, tf.expand_dims(label, axis=-1)\n",
    "\n",
    "\n",
    "normalizer = layers.Rescaling(1.0 / 255.0, name=\"rescale_01\")\n",
    "\n",
    "\n",
    "def normalize(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    return normalizer(image), label\n",
    "\n",
    "\n",
    "augmenter = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.08),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ],\n",
    "    name=\"augmenter\",\n",
    ")\n",
    "\n",
    "\n",
    "def augment(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    return augmenter(image, training=True), label\n",
    "\n",
    "\n",
    "def make_dataset(paths: np.ndarray, labels_arr: np.ndarray) -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels_arr))\n",
    "    return ds.map(load_and_resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_raw = make_dataset(split_paths[\"train\"], split_labels[\"train\"])\n",
    "val_raw = make_dataset(split_paths[\"val\"], split_labels[\"val\"])\n",
    "test_raw = make_dataset(split_paths[\"test\"], split_labels[\"test\"])\n",
    "\n",
    "train_ds = data_utils.prepare_for_training(\n",
    "    train_raw.map(normalize, num_parallel_calls=tf.data.AUTOTUNE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer=SHUFFLE_BUFFER,\n",
    "    augment_fn=augment,\n",
    ")\n",
    "\n",
    "val_ds = data_utils.prepare_for_training(\n",
    "    val_raw.map(normalize, num_parallel_calls=tf.data.AUTOTUNE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer=None,\n",
    ")\n",
    "\n",
    "test_ds = data_utils.prepare_for_training(\n",
    "    test_raw.map(normalize, num_parallel_calls=tf.data.AUTOTUNE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef861373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of augmented training images\n",
    "batch_images, batch_labels = next(iter(train_ds.take(1)))\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "for ax, image, label in zip(axes.flat, batch_images, batch_labels):\n",
    "    ax.imshow(np.clip(image.numpy(), 0, 1))\n",
    "    ax.set_title(class_names[int(label.numpy().squeeze())])\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837aa1e",
   "metadata": {},
   "source": [
    "## Feature extraction with VGG16\n",
    "\n",
    "The VGG16 convolutional base (pre-trained on ImageNet) is loaded without its fully-connected classifier.\n",
    "We freeze all convolutional layers and train a lightweight classification head on top of the frozen features.\n",
    "A `preprocess_input` layer is included inside the model to keep the dataset pipeline agnostic to VGG-specific\n",
    "normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7933d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=IMG_SIZE + (3,),\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "preprocess = layers.Lambda(\n",
    "    keras.applications.vgg16.preprocess_input,\n",
    "    name=\"vgg16_preprocess\",\n",
    ")\n",
    "\n",
    "def build_model(trainable_base: bool = False, learning_rate: float = 1e-4) -> keras.Model:\n",
    "    base_model.trainable = trainable_base\n",
    "    inputs = keras.Input(shape=IMG_SIZE + (3,), name=\"input_image\")\n",
    "    x = preprocess(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"dogs_vs_cats_vgg16\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.AUC(name=\"auc\"),\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(trainable_base=False, learning_rate=1e-3)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c86df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_auc\",\n",
    "        mode=\"max\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "history_feature_extractor = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=initial_epochs,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for the frozen-base stage\n",
    "def plot_history(history: keras.callbacks.History, label: str) -> None:\n",
    "    metrics = [\"loss\", \"accuracy\", \"auc\"]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(18, 4))\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        ax.plot(history.history[metric], label=f\"train {metric}\")\n",
    "        ax.plot(history.history[f\"val_{metric}\"], label=f\"val {metric}\")\n",
    "        ax.set_title(f\"{label} · {metric}\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_history(history_feature_extractor, label=\"Frozen VGG16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e57a31",
   "metadata": {},
   "source": [
    "## Fine-tuning strategy 1 · Unfreeze the final convolutional block\n",
    "\n",
    "With the classifier head trained, we can adapt the deeper convolutional filters.\n",
    "Here we unfreeze **Block 5** of VGG16 (the last convolutional block) and continue training with a\n",
    "smaller learning rate to avoid catastrophic forgetting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last convolutional block (Block 5)\n",
    "block5_start = 15  # index at which block5_conv1 appears in the VGG16 architecture\n",
    "for layer in base_model.layers[:block5_start]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[block5_start:]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "fine_tune_epochs = 5\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_block5 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history_feature_extractor.epoch[-1] + 1,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a40516",
   "metadata": {},
   "source": [
    "## Fine-tuning strategy 2 · Gradual unfreezing of the entire base\n",
    "\n",
    "After adapting the final block, we can slowly unfreeze the rest of VGG16. Lower the learning rate further\n",
    "and optionally use an exponential decay schedule. Monitor validation performance closely to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e72f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the entire base for a final polishing stage\n",
    "base_model.trainable = True\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-6),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history_full = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs + 5,\n",
    "    initial_epoch=history_block5.epoch[-1] + 1,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5f41c",
   "metadata": {},
   "source": [
    "## Evaluation on the hold-out test set\n",
    "\n",
    "Use the test split to obtain an unbiased estimate of the model's generalization performance.\n",
    "This includes aggregate metrics and optional qualitative inspection of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(test_ds, return_dict=True)\n",
    "print(json.dumps(test_metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few predictions on the test set\n",
    "probabilities = []\n",
    "labels_true = []\n",
    "images = []\n",
    "for batch_images, batch_labels in test_ds.take(5):\n",
    "    preds = model.predict(batch_images)\n",
    "    probabilities.extend(np.asarray(preds).squeeze().tolist())\n",
    "    labels_true.extend(batch_labels.numpy().squeeze().tolist())\n",
    "    images.extend(batch_images.numpy())\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "for ax, image, prob, label in zip(axes.flat, images, probabilities, labels_true):\n",
    "    ax.imshow(np.clip(image, 0, 1))\n",
    "    pred_label = class_names[int(prob >= 0.5)]\n",
    "    ax.set_title(f\"Pred: {pred_label} ({prob:.2f})\n",
    "True: {class_names[int(label)]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}