{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25c057c",
   "metadata": {},
   "source": [
    "# W02 · Perceptron & Multilayer Perceptrons\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "- Review the limitations of single-layer perceptrons and motivate deeper architectures.\n",
    "- Practice reasoning about when to add layers versus adding more neurons to existing layers.\n",
    "- Compare common activation functions and recognize how they influence optimization dynamics.\n",
    "- Build intuition for diagnosing underfitting/overfitting when designing multilayer perceptrons (MLPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ff53b",
   "metadata": {},
   "source": [
    "## Recap: From Perceptrons to MLPs\n",
    "\n",
    "In Week 01 we trained a single dense layer on Fashion-MNIST. The perceptron learned a linear decision boundary over flattened pixels.\n",
    "\n",
    "> **Key limitation:** Without hidden layers the model cannot express interactions between pixels. Decision boundaries remain linear, so patterns like curves, textures, or hierarchical shapes are hard to capture.\n",
    "\n",
    "To move beyond linear separability we introduce **hidden layers**. Each additional layer composes nonlinear transformations that can\n",
    "model progressively more abstract features. However, deeper networks add more parameters and optimization challenges, so we need a principled way to decide when to deepen versus widen a network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cad17",
   "metadata": {},
   "source": [
    "## Heuristics for Adding Layers or Neurons\n",
    "\n",
    "When designing an MLP, consider the following heuristics:\n",
    "\n",
    "- **Problem complexity**: If your features interact in hierarchical ways (e.g., strokes composing clothing items), adding layers can help. If interactions are mostly linear, widening layers might suffice.\n",
    "- **Bias vs. variance**:\n",
    "  - High bias / underfitting → add layers or neurons, or change activations to increase capacity.\n",
    "  - High variance / overfitting → reduce layers/neurons, add regularization, or collect more data.\n",
    "- **Representation bottlenecks**: Too few neurons in a critical layer can restrict information flow. Gradually increase width while monitoring validation metrics.\n",
    "- **Training dynamics**: Vanishing gradients or slow convergence could mean the architecture or activation is poorly matched; try `relu` or `elu` for deeper stacks, or add normalization.\n",
    "- **Parameter budget / inference cost**: Deeper and wider networks cost more to train/serve. Aim for the simplest model that meets your performance target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e95ea",
   "metadata": {},
   "source": [
    "## Setup: Imports & Data Pipeline\n",
    "\n",
    "We will reuse the Fashion-MNIST dataset from Week 01. To keep experiments fast, we'll train on a 12k-image subset and hold out 2k examples for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Load Fashion-MNIST\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "train_images = train_images.astype(\"float32\") / 255.0\n",
    "test_images = test_images.astype(\"float32\") / 255.0\n",
    "\n",
    "# Flatten inputs for dense networks\n",
    "train_images = train_images.reshape(-1, 28 * 28)\n",
    "test_images = test_images.reshape(-1, 28 * 28)\n",
    "\n",
    "# Create train/validation split (12k train / 2k val)\n",
    "val_split = 2000\n",
    "train_subset = 12000\n",
    "\n",
    "train_x = train_images[:train_subset]\n",
    "train_y = train_labels[:train_subset]\n",
    "val_x = train_images[train_subset:train_subset + val_split]\n",
    "val_y = train_labels[train_subset:train_subset + val_split]\n",
    "\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "print(f\"Training subset: {train_x.shape}, Validation subset: {val_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15d6cf",
   "metadata": {},
   "source": [
    "## Comparing Architectures: Depth vs. Width\n",
    "\n",
    "We'll train several `tf.keras.Sequential` models that vary in depth (number of hidden layers) and width (neurons per layer). Each model trains for a small number of epochs so we can compare validation performance quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(hidden_layers, activation=\"relu\", dropout_rate=0.0):\n",
    "    model = keras.Sequential(name=\"mlp\")\n",
    "    model.add(layers.Input(shape=(28 * 28,)))\n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "architectures = [\n",
    "    (\"shallow_wide\", [256], \"1 hidden layer, wide\"),\n",
    "    (\"two_layer_balanced\", [256, 128], \"2 hidden layers\"),\n",
    "    (\"deep_narrow\", [128, 128, 64, 64], \"4 hidden layers, narrower\"),\n",
    "]\n",
    "\n",
    "histories = {}\n",
    "for model_key, hidden_layers, description in architectures:\n",
    "    print(f\"\n",
    "Training model: {model_key} ({description})\")\n",
    "    model = build_mlp(hidden_layers)\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    histories[model_key] = history.history\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    print(f\"Final validation accuracy: {val_acc:.3f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "for model_key, history in histories.items():\n",
    "    epochs = range(1, len(history[\"accuracy\"]) + 1)\n",
    "    axes[0].plot(epochs, history[\"accuracy\"], marker=\"o\", label=model_key)\n",
    "    axes[1].plot(epochs, history[\"val_accuracy\"], marker=\"o\", label=model_key)\n",
    "\n",
    "axes[0].set_title(\"Training accuracy\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axes[1].set_title(\"Validation accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7858e",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "- How quickly does each architecture reach strong validation accuracy?\n",
    "- Which model shows signs of underfitting (low training + validation accuracy)?\n",
    "- Do any models hint at overfitting (training accuracy increases while validation plateaus or drops)?\n",
    "\n",
    "Use these observations to reason about when additional layers help versus when simply widening a single layer is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793ac10",
   "metadata": {},
   "source": [
    "## Underfitting vs. Overfitting in Practice\n",
    "\n",
    "Architecture decisions can go wrong in both directions. We'll contrast a tiny network that lacks capacity with a very deep, wide network trained for longer without regularization to show how training and validation metrics diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d150e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extreme_configs = [\n",
    "    (\"underfit_tiny\", [32], \"Tiny capacity (underfitting)\"),\n",
    "    (\"overfit_huge\", [512, 512, 256, 256, 128, 128], \"Deep and wide (overfitting)\"),\n",
    "]\n",
    "\n",
    "extreme_histories = {}\n",
    "for key, hidden_layers, description in extreme_configs:\n",
    "    print(f\"Training extreme model: {description}\")\n",
    "    model = build_mlp(hidden_layers)\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    extreme_histories[key] = history.history\n",
    "    final_train = history.history['accuracy'][-1]\n",
    "    final_val = history.history['val_accuracy'][-1]\n",
    "    print(f\"Final train acc: {final_train:.3f}, val acc: {final_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c7c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_history_curves(histories, title_prefix=\"\"):\n",
    "    metrics = [(\"accuracy\", \"Accuracy\"), (\"loss\", \"Loss\")]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(14, 4))\n",
    "    for ax, (metric, display_name) in zip(axes, metrics):\n",
    "        for key, history in histories.items():\n",
    "            epochs = range(1, len(history[metric]) + 1)\n",
    "            ax.plot(epochs, history[metric], label=f\"{key} train\")\n",
    "            ax.plot(epochs, history[f'val_{metric}'], linestyle='--', label=f\"{key} val\")\n",
    "        ax.set_title(f\"{title_prefix}{display_name}\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(display_name)\n",
    "    handles, labels = axes[-1].get_legend_handles_labels()\n",
    "    axes[-1].legend(handles, labels, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history_curves(extreme_histories, title_prefix=\"Extreme models: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee31b2f",
   "metadata": {},
   "source": [
    "### What do we observe?\n",
    "\n",
    "- **Underfit tiny network**: Training and validation accuracy stay low and the losses remain high, which tells us the model doesn't have enough capacity to represent the patterns in Fashion-MNIST.\n",
    "- **Overfit huge network**: Training accuracy keeps climbing while validation accuracy plateaus and validation loss rises, showing the model memorizes the training subset but generalizes poorly.\n",
    "- **Takeaway**: Effective architectures balance depth and width with regularization. Too few layers underfit, while an overpowered network without safeguards quickly overfits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9157a5",
   "metadata": {},
   "source": [
    "## Activation Function Experiments\n",
    "\n",
    "Activations control how information flows between layers. We'll keep a two-layer architecture fixed and vary the nonlinearities to see how they impact convergence and final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"relu\", \"tanh\", \"sigmoid\", \"elu\"]\n",
    "activation_histories = {}\n",
    "for act in activations:\n",
    "    print(f\"\n",
    "Training activation variant: {act}\")\n",
    "    model = build_mlp([256, 128], activation=act)\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    activation_histories[act] = history.history\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for act, history in activation_histories.items():\n",
    "    epochs = range(1, len(history[\"val_accuracy\"]) + 1)\n",
    "    plt.plot(epochs, history[\"val_accuracy\"], marker=\"o\", label=act)\n",
    "plt.title(\"Validation accuracy by activation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53992d5c",
   "metadata": {},
   "source": [
    "### Activation Takeaways\n",
    "\n",
    "- `relu` (and variants like `elu`) often accelerate training in deeper networks because they mitigate vanishing gradients.\n",
    "- Saturating activations (`sigmoid`, `tanh`) can struggle without careful initialization or normalization but may still perform competitively with fewer layers.\n",
    "- Observe whether activations converge to similar validation accuracy despite different training trajectories.\n",
    "\n",
    "> **Try it:** Mix activations across layers (e.g., first layer `relu`, second `tanh`) and see how the dynamics change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771b99f",
   "metadata": {},
   "source": [
    "## Guided Exercises\n",
    "\n",
    "1. **Architecture tuning**: Modify `architectures` to include a very small network (e.g., `[64]`) and a very deep one (e.g., six layers). Compare validation accuracy and discuss bias/variance trade-offs.\n",
    "2. **Regularization challenge**: Extend the new underfitting/overfitting section by adding dropout, `layers.BatchNormalization()`, or early stopping to the overfitting model. How much can you shrink the validation gap?\n",
    "3. **Activation sweep**: Experiment with `swish` (`tf.nn.swish`) or `gelu`. You can pass a callable to the `activation` argument.\n",
    "4. **TensorBoard logging (optional)**: Wrap training with `keras.callbacks.TensorBoard(log_dir=\"runs/week02\")` to visualize losses and gradients. Record at least one screenshot of your TensorBoard scalars.\n",
    "5. **Reflection**: For each experiment, note (a) the architecture, (b) activation(s), and (c) final training/validation accuracy. Summarize which design choices worked best and why.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
