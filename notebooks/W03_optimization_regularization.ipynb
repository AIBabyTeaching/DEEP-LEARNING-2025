{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e144c8",
   "metadata": {},
   "source": [
    "# W03 \u00b7 Optimization & Regularization\n",
    "\n",
    "This lab investigates how optimization algorithms and regularization strategies work together to improve deep learning models. You will combine theoretical intuition with practical TensorFlow experiments executed through the course utility helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d000cf",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab you will be able to:\n",
    "\n",
    "- Derive and interpret the update rules of popular first-order optimizers.\n",
    "- Explain how regularization mechanisms change the effective hypothesis space of neural networks.\n",
    "- Run disciplined experiments on multiple datasets (including **Fashion-MNIST**) using the course `dl_utils` helpers.\n",
    "- Compare and contrast the impact of optimization versus regularization choices on convergence speed and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Recap: From 1D Toy Example to Real Datasets\n\nIn the theory sheet \"Gradient-Based Optimization and Regularization,\" we explored a simple 1D example with:\n- A single scalar parameter **\u03b8** (theta)\n- Two consecutive gradients: **g\u2081 = 4** and **g\u2082 = 2**\n- A learning rate **\u03b7 = 0.1** (eta)\n- Four optimizer update rules: **SGD**, **Momentum**, **RMSProp**, and **Adam**\n\n### Key Concepts\n\n**Parameter and Gradient:**\n- **\u03b8** (theta): The model parameter we want to optimize\n- **g_t**: The gradient at iteration t, indicating the direction and magnitude of steepest ascent\n- **\u03b7** (eta): The learning rate, controlling the step size\n\n**Optimizers:**\n\n1. **SGD (Stochastic Gradient Descent)**: \n   - Simplest optimizer: \u03b8_{t+1} = \u03b8_t - \u03b7 \u00b7 g_t\n   - Takes fixed-size steps proportional to the gradient\n   \n2. **Momentum**: \n   - Accumulates velocity: v_t = \u03b2 \u00b7 v_{t-1} + g_t\n   - Update: \u03b8_{t+1} = \u03b8_t - \u03b7 \u00b7 v_t\n   - Smooths updates and accelerates in consistent directions\n   \n3. **RMSProp**: \n   - Adapts learning rate per parameter: s_t = \u03b2 \u00b7 s_{t-1} + (1-\u03b2) \u00b7 g_t\u00b2\n   - Update: \u03b8_{t+1} = \u03b8_t - \u03b7 \u00b7 g_t / \u221a(s_t + \u03b5)\n   - Helps with parameters that have different scales\n   \n4. **Adam** (Adaptive Moment Estimation): \n   - Combines momentum and RMSProp\n   - Maintains both first moment (mean) and second moment (variance) estimates\n   - Includes bias correction for early iterations\n\n**Regularization Techniques:**\n\n1. **L2 Weight Decay**: Penalizes large weights by adding \u03bb/2 \u00b7 ||\u03b8||\u00b2 to the loss\n   - Encourages smaller, simpler models\n   - Prevents overfitting by constraining parameter values\n\n2. **Dropout**: Randomly zeros activations during training with probability p\n   - Prevents co-adaptation of neurons\n   - Acts as ensemble learning by training many sub-networks\n\n3. **Data Augmentation**: Creates variations of training data (flips, rotations, crops)\n   - Increases effective training set size\n   - Improves model robustness to transformations\n\n4. **Early Stopping**: Monitors validation performance and stops training when it degrades\n   - Prevents overfitting by limiting training time\n   - Automatically finds the optimal number of epochs\n\n### From Theory to Practice\n\nThe 1D toy example helped us understand the mathematical update rules. Now, we'll apply these same concepts to real neural networks trained on image datasets (Fashion-MNIST). The principles remain the same, but we'll see how they scale to:\n- Thousands or millions of parameters instead of one\n- Mini-batch gradients instead of fixed values\n- Complex loss surfaces instead of simple quadratic functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Toy Example: Numerical Verification\n\nLet's reproduce the 1D example from the theory sheet to connect formulas to code. We'll compute \u03b8 after two gradient steps for each optimizer.\n\n**Setup:**\n- Initial parameter: \u03b8\u2080 = 0\n- Gradients: g\u2081 = 4, g\u2082 = 2\n- Learning rate: \u03b7 = 0.1\n- Hyperparameters: Momentum \u03b2 = 0.9, RMSProp \u03b2 = 0.9, Adam \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999, \u03b5 = 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def toy_optimizer_comparison():\n",
    "    \"\"\"\n",
    "    Reproduce the 1D toy example from the theory sheet.\n",
    "    Computes \u03b8 after two gradient steps for SGD, Momentum, RMSProp, and Adam.\n",
    "    \"\"\"\n",
    "    # Problem setup\n",
    "    theta_0 = 0.0  # Initial parameter value\n",
    "    g1, g2 = 4.0, 2.0  # Two consecutive gradients\n",
    "    eta = 0.1  # Learning rate\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. SGD (Stochastic Gradient Descent)\n",
    "    # Update rule: \u03b8_{t+1} = \u03b8_t - \u03b7 \u00b7 g_t\n",
    "    theta = theta_0\n",
    "    theta = theta - eta * g1  # After first gradient: \u03b8\u2081 = 0 - 0.1 * 4 = -0.4\n",
    "    theta = theta - eta * g2  # After second gradient: \u03b8\u2082 = -0.4 - 0.1 * 2 = -0.6\n",
    "    results['SGD'] = theta\n",
    "    \n",
    "    # 2. Momentum\n",
    "    # Update rule: v_t = \u03b2 \u00b7 v_{t-1} + g_t, \u03b8_{t+1} = \u03b8_t - \u03b7 \u00b7 v_t\n",
    "    beta_momentum = 0.9\n",
    "    theta = theta_0\n",
    "    v = 0.0  # Initial velocity\n",
    "    \n",
    "    v = beta_momentum * v + g1  # v\u2081 = 0.9 * 0 + 4 = 4\n",
    "    theta = theta - eta * v      # \u03b8\u2081 = 0 - 0.1 * 4 = -0.4\n",
    "    \n",
    "    v = beta_momentum * v + g2  # v\u2082 = 0.9 * 4 + 2 = 5.6\n",
    "    theta = theta - eta * v      # \u03b8\u2082 = -0.4 - 0.1 * 5.6 = -0.96\n",
    "    results['Momentum'] = theta\n",
    "    \n",
    "    # 3. RMSProp\n",
    "    # Update rule: s_t = \u03b2 \u00b7 s_{t-1} + (1-\u03b2) \u00b7 g_t\u00b2, \u03b8_{t+1} = \u03b8_t - \u03b7 \u00b7 g_t / \u221a(s_t + \u03b5)\n",
    "    beta_rms = 0.9\n",
    "    epsilon = 1e-8\n",
    "    theta = theta_0\n",
    "    s = 0.0  # Initial squared gradient accumulator\n",
    "    \n",
    "    s = beta_rms * s + (1 - beta_rms) * g1**2  # s\u2081 = 0.9 * 0 + 0.1 * 16 = 1.6\n",
    "    theta = theta - eta * g1 / np.sqrt(s + epsilon)  # \u03b8\u2081 = 0 - 0.1 * 4 / \u221a1.6 \u2248 -0.316\n",
    "    \n",
    "    s = beta_rms * s + (1 - beta_rms) * g2**2  # s\u2082 = 0.9 * 1.6 + 0.1 * 4 = 1.84\n",
    "    theta = theta - eta * g2 / np.sqrt(s + epsilon)  # \u03b8\u2082 \u2248 -0.316 - 0.1 * 2 / \u221a1.84 \u2248 -0.463\n",
    "    results['RMSProp'] = theta\n",
    "    \n",
    "    # 4. Adam (Adaptive Moment Estimation)\n",
    "    # Combines momentum and RMSProp with bias correction\n",
    "    beta1 = 0.9   # First moment decay\n",
    "    beta2 = 0.999  # Second moment decay\n",
    "    epsilon = 1e-8\n",
    "    theta = theta_0\n",
    "    m = 0.0  # First moment estimate\n",
    "    v = 0.0  # Second moment estimate\n",
    "    \n",
    "    # First update\n",
    "    t = 1\n",
    "    m = beta1 * m + (1 - beta1) * g1  # m\u2081 = 0.9 * 0 + 0.1 * 4 = 0.4\n",
    "    v = beta2 * v + (1 - beta2) * g1**2  # v\u2081 = 0.999 * 0 + 0.001 * 16 = 0.016\n",
    "    m_hat = m / (1 - beta1**t)  # Bias correction: m\u0302\u2081 = 0.4 / (1 - 0.9) = 4.0\n",
    "    v_hat = v / (1 - beta2**t)  # Bias correction: v\u0302\u2081 = 0.016 / (1 - 0.999) = 16.0\n",
    "    theta = theta - eta * m_hat / (np.sqrt(v_hat) + epsilon)  # \u03b8\u2081 = 0 - 0.1 * 4 / 4 = -0.1\n",
    "    \n",
    "    # Second update\n",
    "    t = 2\n",
    "    m = beta1 * m + (1 - beta1) * g2  # m\u2082 = 0.9 * 0.4 + 0.1 * 2 = 0.56\n",
    "    v = beta2 * v + (1 - beta2) * g2**2  # v\u2082 = 0.999 * 0.016 + 0.001 * 4 = 0.01998\n",
    "    m_hat = m / (1 - beta1**t)  # Bias correction: m\u0302\u2082 = 0.56 / (1 - 0.81) = 2.947\n",
    "    v_hat = v / (1 - beta2**t)  # Bias correction: v\u0302\u2082 = 0.01998 / (1 - 0.998) \u2248 9.99\n",
    "    theta = theta - eta * m_hat / (np.sqrt(v_hat) + epsilon)  # \u03b8\u2082 \u2248 -0.1 - 0.1 * 2.947 / 3.16 \u2248 -0.193\n",
    "    results['Adam'] = theta\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the toy example\n",
    "toy_results = toy_optimizer_comparison()\n",
    "\n",
    "# Display results in a table\n",
    "toy_df = pd.DataFrame([toy_results], index=['\u03b8\u2082 (final value)'])\n",
    "print(\"\\n1D Toy Example Results (after 2 gradient steps):\")\n",
    "print(\"=\"*60)\n",
    "display(toy_df)\n",
    "\n",
    "print(\"\\n\u2713 These values match the computational pattern from theory:\")\n",
    "print(\"  - SGD: \u03b8\u2082 = -0.60 (straightforward gradient descent)\")\n",
    "print(\"  - Momentum: \u03b8\u2082 = -0.96 (faster convergence due to velocity)\")\n",
    "print(\"  - RMSProp: \u03b8\u2082 \u2248 -0.46 (adaptive step size)\")\n",
    "print(\"  - Adam: \u03b8\u2082 \u2248 -0.19 (combines both with bias correction)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Fashion-MNIST\n\nFor the main experiments, we'll use **Fashion-MNIST**, a dataset of 70,000 grayscale images (28\u00d728 pixels) across 10 clothing categories.\n\n**Task:** 10-class image classification\n**Classes:** T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n\n**Why Fashion-MNIST?**\n- More challenging than MNIST digits (requires learning more complex features)\n- Same format as MNIST (easy to work with)\n- Fast to train (small images, manageable dataset size)\n- Good for demonstrating optimizer and regularization effects\n\n**Data Splits:**\n- Training: 54,000 images (90% of training set)\n- Validation: 6,000 images (10% of training set)\n- Test: 10,000 images (separate test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for the lab\n",
    "from __future__ import annotations  # Enable postponed evaluation of annotations\n",
    "\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data analysis and results summarization\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import tensorflow as tf  # Deep learning framework\n",
    "from tensorflow import keras  # High-level neural networks API\n",
    "from IPython.display import display, Markdown  # Notebook display utilities\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "val_size = 6000\n",
    "x_train, x_val = x_train_full[:-val_size], x_train_full[-val_size:]\n",
    "y_train, y_val = y_train_full[:-val_size], y_train_full[-val_size:]\n",
    "\n",
    "# Normalize pixel values to [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_val = x_val.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for CNN compatibility\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_val = x_val[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training set: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {x_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {x_test.shape}, {y_test.shape}\")\n",
    "print(f\"Input shape: {x_train.shape[1:]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Display a few sample images\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(x_train[i, :, :, 0], cmap='gray')\n",
    "    plt.title(class_names[y_train[i]], fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable Model Builder\n\nThis function builds a CNN model with configurable optimizer, regularization, and data augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape=(28, 28, 1),\n",
    "    num_classes=10,\n",
    "    base_width=64,\n",
    "    optimizer_name=\"sgd\",\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.0,\n",
    "    dropout_rate=0.0,\n",
    "    use_data_augmentation=False,\n",
    "):\n",
    "    # Build a CNN model with configurable optimization and regularization.\n",
    "    # Set up L2 regularizer if weight_decay > 0\n",
    "    regularizer = keras.regularizers.l2(weight_decay) if weight_decay > 0 else None\n",
    "    \n",
    "    # Build model architecture\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Optional: Add data augmentation layers at the input\n",
    "    if use_data_augmentation:\n",
    "        x = keras.layers.RandomFlip(\"horizontal\")(x)\n",
    "        x = keras.layers.RandomRotation(0.1)(x)\n",
    "    \n",
    "    # Convolutional block 1\n",
    "    x = keras.layers.Conv2D(\n",
    "        base_width, \n",
    "        kernel_size=3, \n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Convolutional block 2\n",
    "    x = keras.layers.Conv2D(\n",
    "        base_width * 2, \n",
    "        kernel_size=3, \n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer)(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Configure optimizer\n",
    "    if optimizer_name == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"momentum\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the model builder\n",
    "test_model = build_model()\n",
    "print(f\"Model built successfully!\")\n",
    "print(f\"Total parameters: {test_model.count_params():,}\")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,  # Compiled Keras model\n",
    "    x_train, y_train,  # Training data\n",
    "    x_val, y_val,  # Validation data\n",
    "    batch_size=128,  # Batch size for training\n",
    "    max_epochs=20,  # Maximum number of epochs\n",
    "    use_early_stopping=True,  # Whether to use early stopping\n",
    "    patience=3,  # Patience for early stopping\n",
    "    verbose=1,  # Verbosity level (0=silent, 1=progress bar, 2=one line per epoch)\n",
    "):\n",
    "    # Train a Keras model with optional early stopping.\n",
    "    # Set up callbacks\n",
    "    callbacks = []\n",
    "    \n",
    "    if use_early_stopping:\n",
    "        # Early stopping: monitor validation loss, restore best weights\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,  # Restore model to best epoch\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment Grid\n\nWe'll run a carefully selected set of experiments to demonstrate the effects of different optimizers and regularization techniques. Each experiment varies one or two parameters to isolate their impact.\n\n**Experiment Design:**\n1. **Baseline comparisons** - Different optimizers without regularization\n2. **L2 weight decay** - Impact of weight regularization\n3. **Dropout** - Impact of stochastic regularization\n4. **Data augmentation** - Impact of input transformations\n5. **Combined regularization** - Multiple techniques together\n6. **Early stopping** - Impact on preventing overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "# Each experiment varies specific parameters to isolate their effects\n",
    "\n",
    "experiments = [\n",
    "    # 1. Optimizer comparisons (no regularization)\n",
    "    {\n",
    "        'name': 'SGD_baseline',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,  # Higher LR for SGD\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Momentum_baseline',\n",
    "        'optimizer': 'momentum',\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'RMSProp_baseline',\n",
    "        'optimizer': 'rmsprop',\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Adam_baseline',\n",
    "        'optimizer': 'adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': True,\n",
    "    },\n",
    "    \n",
    "    # 2. SGD with L2 weight decay\n",
    "    {\n",
    "        'name': 'SGD_L2',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 1e-4,  # Add L2 regularization\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': True,\n",
    "    },\n",
    "    \n",
    "    # 3. SGD with dropout\n",
    "    {\n",
    "        'name': 'SGD_dropout',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout_rate': 0.3,  # Add dropout\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': True,\n",
    "    },\n",
    "    \n",
    "    # 4. Adam with combined regularization\n",
    "    {\n",
    "        'name': 'Adam_full_regularization',\n",
    "        'optimizer': 'adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-4,  # L2 regularization\n",
    "        'dropout_rate': 0.3,   # Dropout\n",
    "        'use_data_augmentation': True,  # Data augmentation\n",
    "        'use_early_stopping': True,  # Early stopping\n",
    "    },\n",
    "    \n",
    "    # 5. Adam without early stopping (to show overfitting)\n",
    "    {\n",
    "        'name': 'Adam_no_early_stop',\n",
    "        'optimizer': 'adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_data_augmentation': False,\n",
    "        'use_early_stopping': False,  # No early stopping\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(experiments)} experiments\")\n",
    "print(\"\\nExperiment configurations:\")\n",
    "for i, exp in enumerate(experiments, 1):\n",
    "    print(f\"{i}. {exp['name']}: optimizer={exp['optimizer']}, \"\n",
    "          f\"L2={exp['weight_decay']}, dropout={exp['dropout_rate']}, \"\n",
    "          f\"augment={exp['use_data_augmentation']}, \"\n",
    "          f\"early_stop={exp['use_early_stopping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Experiments\n\nNow we'll train each model and collect the results. This may take several minutes depending on your hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all experiments\n",
    "all_results = []\n",
    "\n",
    "for i, config in enumerate(experiments, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Experiment {i}/{len(experiments)}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Build model with current configuration\n",
    "    model = build_model(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        num_classes=10,\n",
    "        base_width=64,\n",
    "        optimizer_name=config['optimizer'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        use_data_augmentation=config['use_data_augmentation'],\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(\n",
    "        model,\n",
    "        x_train, y_train_cat,\n",
    "        x_val, y_val_cat,\n",
    "        batch_size=128,\n",
    "        max_epochs=20,\n",
    "        use_early_stopping=config['use_early_stopping'],\n",
    "        patience=3,\n",
    "        verbose=2  # One line per epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'config': config,\n",
    "        'history': history.history,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'best_val_acc': max(history.history['val_accuracy']),\n",
    "        'epochs_trained': len(history.history['accuracy'])\n",
    "    }\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\n\u2713 Final training accuracy: {result['final_train_acc']:.4f}\")\n",
    "    print(f\"\u2713 Final validation accuracy: {result['final_val_acc']:.4f}\")\n",
    "    print(f\"\u2713 Best validation accuracy: {result['best_val_acc']:.4f}\")\n",
    "    print(f\"\u2713 Test accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"\u2713 Epochs trained: {result['epochs_trained']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n\nLet's create a comprehensive table comparing all experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    config = result['config']\n",
    "    summary_data.append({\n",
    "        'Experiment': config['name'],\n",
    "        'Optimizer': config['optimizer'],\n",
    "        'Learning Rate': config['learning_rate'],\n",
    "        'L2 Weight Decay': config['weight_decay'],\n",
    "        'Dropout': config['dropout_rate'],\n",
    "        'Data Aug': 'Yes' if config['use_data_augmentation'] else 'No',\n",
    "        'Early Stop': 'Yes' if config['use_early_stopping'] else 'No',\n",
    "        'Epochs': result['epochs_trained'],\n",
    "        'Final Train Acc': f\"{result['final_train_acc']:.4f}\",\n",
    "        'Final Val Acc': f\"{result['final_val_acc']:.4f}\",\n",
    "        'Best Val Acc': f\"{result['best_val_acc']:.4f}\",\n",
    "        'Test Acc': f\"{result['test_accuracy']:.4f}\",\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nExperiment Results Summary:\")\n",
    "print(\"=\"*120)\n",
    "display(results_df)\n",
    "\n",
    "# Find best performing model\n",
    "best_idx = np.argmax([r['test_accuracy'] for r in all_results])\n",
    "best_name = all_results[best_idx]['config']['name']\n",
    "best_acc = all_results[best_idx]['test_accuracy']\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best performing model: {best_name} with test accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Training Curves\n\nLet's plot the training and validation accuracy curves to see how different configurations affect learning dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all experiments\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    if idx < len(axes):\n",
    "        ax = axes[idx]\n",
    "        config = result['config']\n",
    "        history = result['history']\n",
    "        \n",
    "        # Plot training and validation accuracy\n",
    "        epochs = range(1, len(history['accuracy']) + 1)\n",
    "        ax.plot(epochs, history['accuracy'], 'b-', label='Training', linewidth=2)\n",
    "        ax.plot(epochs, history['val_accuracy'], 'r-', label='Validation', linewidth=2)\n",
    "        \n",
    "        # Mark best validation epoch\n",
    "        best_epoch = np.argmax(history['val_accuracy']) + 1\n",
    "        best_val = max(history['val_accuracy'])\n",
    "        ax.plot(best_epoch, best_val, 'r*', markersize=15, label=f'Best (epoch {best_epoch})')\n",
    "        \n",
    "        ax.set_title(f\"{config['name']}\\nTest Acc: {result['test_accuracy']:.4f}\", \n",
    "                    fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0.7, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Test Accuracy Comparison\n\nA bar chart makes it easy to compare final test accuracy across all experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart comparing test accuracies\n",
    "names = [r['config']['name'] for r in all_results]\n",
    "test_accs = [r['test_accuracy'] for r in all_results]\n",
    "val_accs = [r['best_val_acc'] for r in all_results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, val_accs, width, label='Best Val Accuracy', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Experiment', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Validation and Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0.8, 0.95])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation: Linking Back to the 1D Example\n\nNow let's connect our experimental results back to the theory and the 1D toy example.\n\n### Optimizer Observations\n\n**From the 1D Example:**\n- SGD took straightforward steps: \u03b8\u2082 = -0.60\n- Momentum accumulated velocity and moved faster: \u03b8\u2082 = -0.96\n- RMSProp adapted its step size: \u03b8\u2082 = -0.46\n- Adam combined both strategies with bias correction: \u03b8\u2082 = -0.19\n\n**In Our Fashion-MNIST Experiments:**\n\n1. **SGD** is the simplest but often requires more epochs to converge. It follows the raw gradient direction without smoothing or adaptation. The learning rate must be carefully tuned.\n\n2. **Momentum** (SGD with velocity) accelerates training by accumulating gradients over time. Like a ball rolling down a hill, it builds up speed in consistent directions. This helps:\n   - Escape from plateaus faster\n   - Smooth out noisy gradients\n   - Converge more quickly than vanilla SGD\n\n3. **RMSProp** adapts the learning rate for each parameter based on recent gradient magnitudes. This is particularly helpful when:\n   - Different parameters have different scales\n   - Some directions have much steeper gradients than others\n   - The loss surface has different curvatures in different directions\n\n4. **Adam** combines the best of both worlds:\n   - Momentum for smoothing and acceleration\n   - RMSProp's adaptive learning rates\n   - Bias correction for accurate estimates early in training\n   - Generally converges fastest and most reliably\n\n**Key Insight:** Just as Adam took smaller, more carefully calibrated steps in the 1D example (\u03b8\u2082 \u2248 -0.19 vs SGD's -0.60), it also converges efficiently on the complex, high-dimensional Fashion-MNIST loss surface.\n\n### Regularization Observations\n\n**L2 Weight Decay:**\n- Penalizes large weights by adding \u03bb/2 \u00b7 ||\u03b8||\u00b2 to the loss\n- Forces the model to prefer smaller parameter values\n- Reduces overfitting by constraining the hypothesis space\n- In our experiments: Modest improvement in validation/test accuracy\n\n**Dropout:**\n- Randomly drops neurons during training (sets activations to zero)\n- Prevents co-adaptation (neurons can't rely too heavily on specific other neurons)\n- Acts like training an ensemble of many sub-networks\n- In our experiments: Clear reduction in overfitting, especially with Adam\n\n**Data Augmentation:**\n- Creates variations of training images (flips, rotations)\n- Effectively increases dataset size\n- Teaches the model to be invariant to these transformations\n- In our experiments: Further improves generalization\n\n**Early Stopping:**\n- Monitors validation loss and stops when it stops improving\n- Prevents the model from overfitting by training too long\n- Automatically finds the optimal number of epochs\n- In our experiments: Essential for preventing divergence and overfitting\n\n### Combined Effects\n\nThe experiment **Adam_full_regularization** (L2 + Dropout + Augmentation + Early Stopping) demonstrates how multiple regularization techniques can work together:\n- Training is slower and more careful\n- Validation and test accuracy are closer (less overfitting)\n- The model generalizes better to unseen data\n\nThe experiment **Adam_no_early_stop** shows what happens without early stopping:\n- Training continues even after validation accuracy plateaus\n- Risk of overfitting increases\n- Gap between training and validation accuracy widens\n\n### Connection to Theory\n\nThe 1D toy example helped us understand:\n- How update rules transform gradients into parameter changes\n- Why momentum accelerates convergence\n- How adaptive methods (RMSProp, Adam) scale steps appropriately\n\nOur Fashion-MNIST experiments demonstrate:\n- These same principles work on real, high-dimensional problems\n- Regularization is crucial for generalization in practice\n- Combining multiple techniques (optimizer + regularizers) yields best results\n\n**Bottom Line:** The mathematical intuition from the 1D example directly translates to practical deep learning. Understanding the update rules helps us choose and tune optimizers effectively, while regularization techniques prevent our models from memorizing training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Interactive Experiments\n\nUse this function to quickly run custom experiments with different configurations. Perfect for classroom demos and exploration!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    optimizer_name=\"adam\",  # Optimizer: \"sgd\", \"momentum\", \"rmsprop\", \"adam\"\n",
    "    learning_rate=0.001,  # Learning rate\n",
    "    weight_decay=0.0,  # L2 regularization strength\n",
    "    dropout_rate=0.0,  # Dropout probability\n",
    "    use_data_augmentation=False,  # Whether to use data augmentation\n",
    "    use_early_stopping=True,  # Whether to use early stopping\n",
    "    base_width=64,  # Number of filters in first conv layer\n",
    "    max_epochs=20,  # Maximum number of epochs\n",
    "    batch_size=128,  # Batch size\n",
    "    show_plot=True,  # Whether to show training curves\n",
    "):\n",
    "    # Build, train, and evaluate a model with custom settings.\n",
    "    # This helper function is designed for interactive experimentation and\n",
    "    # classroom demonstrations.\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CUSTOM EXPERIMENT CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Optimizer:          {optimizer_name}\")\n",
    "    print(f\"Learning Rate:      {learning_rate}\")\n",
    "    print(f\"L2 Weight Decay:    {weight_decay}\")\n",
    "    print(f\"Dropout Rate:       {dropout_rate}\")\n",
    "    print(f\"Data Augmentation:  {use_data_augmentation}\")\n",
    "    print(f\"Early Stopping:     {use_early_stopping}\")\n",
    "    print(f\"Base Width:         {base_width}\")\n",
    "    print(f\"Max Epochs:         {max_epochs}\")\n",
    "    print(f\"Batch Size:         {batch_size}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        num_classes=10,\n",
    "        base_width=base_width,\n",
    "        optimizer_name=optimizer_name,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_data_augmentation=use_data_augmentation,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel has {model.count_params():,} parameters\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining...\")\n",
    "    history = train_model(\n",
    "        model,\n",
    "        x_train, y_train_cat,\n",
    "        x_val, y_val_cat,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=max_epochs,\n",
    "        use_early_stopping=use_early_stopping,\n",
    "        patience=3,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Epochs trained:            {len(history.history['accuracy'])}\")\n",
    "    print(f\"Final training accuracy:   {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Best validation accuracy:  {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Test accuracy:             {test_acc:.4f}\")\n",
    "    print(f\"Test loss:                 {test_loss:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Plot if requested\n",
    "    if show_plot:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax1.plot(epochs, history.history['accuracy'], 'b-', label='Training', linewidth=2)\n",
    "        ax1.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation', linewidth=2)\n",
    "        best_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
    "        best_val = max(history.history['val_accuracy'])\n",
    "        ax1.plot(best_epoch, best_val, 'r*', markersize=15)\n",
    "        ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss plot\n",
    "        ax2.plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=2)\n",
    "        ax2.plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "        ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\nExample: Run an experiment with Adam and full regularization\")\n",
    "print(\"Uncomment the lines below to try it:\\n\")\n",
    "print(\"# result = run_experiment(\")\n",
    "print(\"#     optimizer_name='adam',\")\n",
    "print(\"#     learning_rate=0.001,\")\n",
    "print(\"#     weight_decay=1e-4,\")\n",
    "print(\"#     dropout_rate=0.3,\")\n",
    "print(\"#     use_data_augmentation=True,\")\n",
    "print(\"#     use_early_stopping=True\")\n",
    "print(\"# )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Checks\n\n**Question 1.** How does adding momentum change the update direction compared with vanilla SGD?\n\n<details>\n<summary>Hint</summary>\nMomentum forms a running average of recent gradients before applying the step.\n</details>\n\n<details>\n<summary>Answer</summary>\nThe update moves along v_t, a smoothed combination of past gradients, so directions that persist over multiple steps are amplified while oscillations cancel out. In the 1D example, momentum accumulated velocity (v\u2082 = 5.6) leading to \u03b8\u2082 = -0.96, moving further than SGD's -0.60.\n</details>\n\n**Question 2.** Why can L2 weight decay improve generalization even though it constrains the model?\n\n<details>\n<summary>Hint</summary>\nThink about the bias-variance trade-off and Occam's razor.\n</details>\n\n<details>\n<summary>Answer</summary>\nBy penalizing large weights, L2 regularization encourages simpler models that are less likely to fit noise in the training data. This increases bias slightly but reduces variance significantly, leading to better performance on unseen test data. It implements a soft version of Occam's razor: prefer simpler explanations (smaller weights) when they fit the data adequately.\n</details>\n\n**Question 3.** In the 1D toy example, why did Adam take a much smaller step (\u03b8\u2082 \u2248 -0.19) compared to SGD (\u03b8\u2082 = -0.60)?\n\n<details>\n<summary>Hint</summary>\nConsider how Adam's bias correction and adaptive learning rates affect the step size.\n</details>\n\n<details>\n<summary>Answer</summary>\nAdam adapts the learning rate by dividing by the square root of the accumulated squared gradients. With large gradients (g\u2081=4, g\u2082=2), the denominator grows quickly, making the effective step size smaller and more conservative. Additionally, bias correction in early iterations prevents taking overly large steps. This adaptive behavior helps Adam converge stably even with varying gradient magnitudes.\n</details>\n\n**Question 4.** How does dropout act as an ensemble method during training?\n\n<details>\n<summary>Hint</summary>\nEach training batch effectively uses a different sub-network.\n</details>\n\n<details>\n<summary>Answer</summary>\nDropout randomly zeros out different neurons on each forward pass, creating a different sub-network architecture for each batch. Over training, this is equivalent to training an ensemble of 2^n different networks (where n is the number of dropout-able units) that share weights. At test time, using all neurons with their weights scaled appropriately approximates averaging predictions from this ensemble, leading to more robust predictions.\n</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n\n1. **Optimizer Tuning Challenge:** Extend the optimizer sweep to include RMSProp with different learning rates (0.0001, 0.001, 0.01). Plot the learning curves and discuss how learning rate affects RMSProp's performance. Compare it to Adam with the same learning rates.\n\n2. **Regularization Grid Search:** For the Adam optimizer, run a 3\u00d73 grid search over dropout rates (0.0, 0.3, 0.5) and L2 penalties (0.0, 1e-4, 1e-3). Create a heatmap showing test accuracy for each combination. Which combination works best and why?\n\n3. **Data Augmentation Exploration:** Modify the `build_model` function to add more aggressive data augmentation (larger rotations, random zoom, random translation). Train a model with these augmentations and compare to the baseline. Does more augmentation always help? When might it hurt performance?\n\n4. **1D Example Extension:** Extend the 1D toy example to 5 gradient steps instead of 2. Compute \u03b8\u2085 for each optimizer and plot the trajectory of \u03b8 over time. Which optimizer converges fastest? Which is most stable?\n\n5. **Cross-Dataset Generalization:** Repeat the main experiments on MNIST digits instead of Fashion-MNIST. Do the relative rankings of optimizers and regularizers change? What does this tell you about the robustness of these techniques?\n\n6. **Early Stopping Analysis:** Modify the early stopping patience parameter (try 1, 3, 5, 10). How does patience affect final model performance? Is there an optimal patience value, or does it depend on the optimizer and regularization used?\n\n7. **From Scratch Implementation:** Implement the SGD, Momentum, and Adam optimizers from scratch (not using Keras/TensorFlow built-ins) and apply them to training a simple 2-layer MLP on MNIST. Verify that your implementations match the built-in optimizer performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}