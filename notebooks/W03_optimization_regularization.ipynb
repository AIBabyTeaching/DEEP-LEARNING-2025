{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e144c8",
   "metadata": {},
   "source": [
    "# W03 \u00b7 Optimization & Regularization\n",
    "\n",
    "This lab investigates how optimization algorithms and regularization strategies work together to improve deep learning models. You will combine theoretical intuition with practical TensorFlow experiments executed through the course utility helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d000cf",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab you will be able to:\n",
    "\n",
    "- Derive and interpret the update rules of popular first-order optimizers.\n",
    "- Explain how regularization mechanisms change the effective hypothesis space of neural networks.\n",
    "- Run disciplined experiments on multiple datasets (including **Fashion-MNIST**) using the course `dl_utils` helpers.\n",
    "- Compare and contrast the impact of optimization versus regularization choices on convergence speed and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b005c9",
   "metadata": {},
   "source": [
    "## Optimization Refresher\n",
    "\n",
    "The core of gradient-based learning is the iterative parameter update\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta_t),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $\\nabla_\\theta \\mathcal{L}(\\theta_t)$ is the gradient of the loss at iteration $t$. Several refinements build on this primitive:\n",
    "\n",
    "- **Momentum SGD** accumulates an exponential moving average of gradients, creating a velocity term $v_t$ for $g_t = \\nabla_\\theta \\mathcal{L}(\\theta_t)$:\n",
    "  $$\\begin{aligned}\n",
    "  v_t &= \\beta v_{t-1} + (1 - \\beta) g_t, \\\\n\n",
    "  \\theta_{t+1} &= \\theta_t - \\eta \\, v_t.\n",
    "  \\end{aligned}$$\n",
    "- **RMSProp** rescales the learning rate per-parameter using a running estimate of squared gradients $s_t$:\n",
    "  $$\\begin{aligned}\n",
    "  s_t &= \\beta s_{t-1} + (1 - \\beta) g_t^2, \\\\n\n",
    "  \\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\, g_t.\n",
    "  \\end{aligned}$$\n",
    "- **Adam** combines momentum and RMSProp with bias corrections:\n",
    "  $$\\begin{aligned}\n",
    "  m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t, \\\\n\n",
    "  v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2, \\\\n\n",
    "  \\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t}, \\\\n\n",
    "  \\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t}, \\\\n\n",
    "  \\theta_{t+1} &= \\theta_t - \\eta \\, \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}.\n",
    "  \\end{aligned}$$\n",
    "\n",
    "These optimizers differ in how aggressively they adapt the step size and in their sensitivity to poorly scaled gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c66fa6",
   "metadata": {},
   "source": [
    "## Regularization Refresher\n",
    "\n",
    "Regularization constrains the model to favor simpler or more robust solutions:\n",
    "\n",
    "- **$L_2$ weight decay** penalizes large weights by adding $\\frac{\\lambda}{2} \\lVert \\theta \\rVert_2^2$ to the loss, leading to the modified gradient\n",
    "  $$\\nabla_\\theta \\mathcal{L}_{\\text{reg}} = \\nabla_\\theta \\mathcal{L} + \\lambda \\theta.$$\n",
    "- **Dropout** randomly zeros activations with probability $p$ during training, sampling subnetworks that collectively prevent co-adaptation. The expected activation becomes $(1-p) h$ at inference time.\n",
    "- **Data augmentation** synthesizes additional training examples $\\tilde{x}$ sampled from transformations $T(x)$ that preserve labels, effectively enlarging the dataset support.\n",
    "- **Early stopping** halts training when validation loss stops improving, implicitly restricting the number of optimization steps and preventing overfitting.\n",
    "\n",
    "Each technique balances the bias\u2013variance trade-off differently, shaping how well the model generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435bdc3",
   "metadata": {},
   "source": [
    "## Technique Comparison Cheat Sheet\n",
    "\n",
    "| Category | Technique | Primary Benefit | Key Hyperparameters | Typical Trade-offs |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Optimizer | SGD + Momentum | Smooths noisy gradients and accelerates along ravines | Learning rate $\\eta$, momentum $\\beta$ | Sensitive to $\\eta$; may stagnate on plateaus |\n",
    "| Optimizer | RMSProp | Adapts step sizes using squared-gradient averages | $\\eta$, decay $\\beta$, $\\epsilon$ | Can forget long-term gradient information |\n",
    "| Optimizer | Adam | Combines momentum and adaptivity for fast convergence | $\\eta$, $\\beta_1$, $\\beta_2$, $\\epsilon$ | May generalize worse than SGD on some tasks |\n",
    "| Regularization | $L_2$ Weight Decay | Shrinks weights to reduce variance | Penalty $\\lambda$ | Excessive decay underfits |\n",
    "| Regularization | Dropout | Prevents co-adaptation of units | Drop probability $p$ | Slows convergence; tuning $p$ is task-dependent |\n",
    "| Regularization | Data Augmentation | Expands dataset support to improve invariances | Transform family $T$, augmentation strength | Poorly chosen transforms can hurt accuracy |\n",
    "| Regularization | Early Stopping | Stops over-training based on validation metrics | Patience, monitored metric | Requires reliable validation signal |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98382eac",
   "metadata": {},
   "source": [
    "## Lab Roadmap\n",
    "\n",
    "1. Configure the environment and import the reusable helpers from `notebooks/dl_utils`.\n",
    "2. Build shared dataset pipelines for **Fashion-MNIST** and **MNIST**.\n",
    "3. Define a compact multilayer perceptron with configurable regularizers.\n",
    "4. Run optimization-focused experiments (SGD vs. Momentum vs. Adam) on Fashion-MNIST.\n",
    "5. Run regularization-focused experiments (dropout, weight decay, augmentation) on MNIST.\n",
    "6. Summarize and visualize the results, then reflect on the optimizer/regularizer interplay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b226e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # Enable postponed evaluation of annotations for typing simplicity\n",
    "\n",
    "from typing import Any  # Provide flexible type annotations\n",
    "\n",
    "import numpy as np  # Support numerical array operations\n",
    "import tensorflow as tf  # Deep learning framework used throughout the lab\n",
    "import tensorflow_datasets as tfds  # Access to standard benchmark datasets\n",
    "\n",
    "from dl_utils import (  # Import shared deep-learning utilities\n",
    "    build_callbacks,  # Helper to create training callbacks\n",
    "    compile_and_fit,  # Helper to compile and train models\n",
    "    load_tfds_dataset,  # Helper to download TensorFlow Datasets\n",
    "    plot_history,  # Helper to visualize training history\n",
    "    prepare_for_training,  # Helper to prepare tf.data pipelines\n",
    "    summarize_history,  # Helper to summarize metric trends\n",
    ")\n",
    "\n",
    "SEED = 42  # Fixed random seed for reproducibility\n",
    "np.random.seed(SEED)  # Seed NumPy's random number generator\n",
    "tf.random.set_seed(SEED)  # Seed TensorFlow's random number generator\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")  # Display the TensorFlow version in use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8c842",
   "metadata": {},
   "source": [
    "### Dataset Preparation Helpers\n",
    "\n",
    "The helpers below rely on `dl_utils.load_tfds_dataset` and `dl_utils.prepare_for_training` to build consistent `tf.data` pipelines with optional augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940deedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No additional helper functions are defined in this cell to keep the workflow simple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14972a",
   "metadata": {},
   "source": [
    "### Model Builder\n",
    "\n",
    "We use a compact multilayer perceptron with tunable dropout and $L_2$ weight decay so that optimizer and regularization effects remain visible within a few epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_model(\n",
    "    input_shape: tuple[int, ...],  # Shape of the input images\n",
    "    num_classes: int,  # Number of output classes\n",
    "    *,\n",
    "    dropout_rate: float = 0.3,  # Dropout rate for regularization\n",
    "    l2_factor: float = 1e-4,  # Strength of L2 weight decay\n",
    ") -> tf.keras.Model:  # Return a compiled Keras model\n",
    "    regularizer = tf.keras.regularizers.l2(l2_factor) if l2_factor else None  # Optional L2 regularizer\n",
    "\n",
    "    model = tf.keras.Sequential(  # Build a simple sequential network\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=input_shape),  # Define expected input shape\n",
    "            tf.keras.layers.Flatten(),  # Flatten images into vectors\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer),  # First dense layer\n",
    "            tf.keras.layers.Dropout(dropout_rate),  # Dropout after first dense layer\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer),  # Second dense layer\n",
    "            tf.keras.layers.Dropout(dropout_rate / 2.0 if dropout_rate else 0.0),  # Lighter dropout after second layer\n",
    "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),  # Output layer with class probabilities\n",
    "        ]\n",
    "    )\n",
    "    return model  # Return the constructed model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda28bf",
   "metadata": {},
   "source": [
    "### Experiment Management\n",
    "\n",
    "The experiment runner stitches together dataset loading, model creation, compilation, and evaluation. Results are stored in a structured dictionary for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config: dict[str, Any]) -> dict[str, Any]:  # Execute one experiment based on the provided configuration\n",
    "    print(f\"\u25b6 Running experiment: {config['name']}\")  # Show which experiment is running\n",
    "    train_raw, info = load_tfds_dataset(config[\"dataset\"], split=\"train[:90%]\", with_info=True)  # Load most of the training split together with dataset metadata\n",
    "    val_raw = load_tfds_dataset(config[\"dataset\"], split=\"train[90%:]\")  # Load the remaining portion of the training split for validation\n",
    "    test_raw = load_tfds_dataset(config[\"dataset\"], split=\"test\")  # Load the test split for final evaluation\n",
    "    if config.get(\"max_train_size\"):  # If a maximum training size is specified\n",
    "        train_raw = train_raw.take(config[\"max_train_size\"])  # Limit the training dataset size\n",
    "    if config.get(\"max_val_size\"):  # If a maximum validation size is specified\n",
    "        val_raw = val_raw.take(config[\"max_val_size\"])  # Limit the validation dataset size\n",
    "    if config.get(\"max_test_size\"):  # If a maximum test size is specified\n",
    "        test_raw = test_raw.take(config[\"max_test_size\"])  # Limit the test dataset size\n",
    "\n",
    "    def preprocess(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:  # Normalize and format images\n",
    "        image = tf.cast(image, tf.float32) / 255.0  # Scale pixel values to the [0, 1] range\n",
    "        if image.shape.rank == 2:  # Handle grayscale images without a channel dimension\n",
    "            image = tf.expand_dims(image, -1)  # Add a channel dimension for grayscale images\n",
    "        return image, label  # Return the processed image-label pair\n",
    "\n",
    "    train_ds = train_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing to the training data\n",
    "    val_ds = val_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing to the validation data\n",
    "    test_ds = test_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing to the test data\n",
    "\n",
    "    augment_fn = None  # Default to no data augmentation\n",
    "    if config.get(\"augment\"):  # If augmentation is requested\n",
    "        def augment_fn(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:  # Define a simple augmentation routine\n",
    "            image = tf.image.random_flip_left_right(image)  # Randomly flip images horizontally\n",
    "            return image, label  # Return the augmented image-label pair\n",
    "\n",
    "    train_prepared = prepare_for_training(  # Build the training pipeline\n",
    "        train_ds,  # Use the preprocessed training dataset\n",
    "        batch_size=config.get(\"batch_size\", 64),  # Use the configured batch size\n",
    "        shuffle_buffer=2048,  # Shuffle training examples\n",
    "        augment_fn=augment_fn,  # Apply augmentation when enabled\n",
    "    )\n",
    "    val_prepared = prepare_for_training(  # Build the validation pipeline\n",
    "        val_ds,  # Use the preprocessed validation dataset\n",
    "        batch_size=config.get(\"batch_size\", 64),  # Match the training batch size\n",
    "        shuffle_buffer=None,  # Keep validation data in order\n",
    "        cache=True,  # Cache validation batches for efficiency\n",
    "        prefetch=True,  # Prefetch validation batches\n",
    "    )\n",
    "    test_prepared = prepare_for_training(  # Build the test pipeline\n",
    "        test_ds,  # Use the preprocessed test dataset\n",
    "        batch_size=config.get(\"batch_size\", 64),  # Match the training batch size\n",
    "        shuffle_buffer=None,  # Keep test data in order\n",
    "        cache=True,  # Cache test batches for efficiency\n",
    "        prefetch=True,  # Prefetch test batches\n",
    "    )\n",
    "\n",
    "    input_shape = info.features[\"image\"].shape  # Read the image shape from dataset metadata\n",
    "    num_classes = info.features[\"label\"].num_classes  # Read the number of classes from metadata\n",
    "\n",
    "    model = build_baseline_model(  # Construct the baseline model\n",
    "        input_shape,  # Provide the image shape\n",
    "        num_classes,  # Provide the number of classes\n",
    "        dropout_rate=config.get(\"dropout_rate\", 0.3),  # Configure dropout rate\n",
    "        l2_factor=config.get(\"l2_factor\", 0.0),  # Configure L2 regularization\n",
    "    )\n",
    "\n",
    "    callbacks, log_dir = build_callbacks(  # Prepare training callbacks and log directory\n",
    "        experiment_name=config[\"name\"],  # Name the experiment for logging\n",
    "        tensorboard=False,  # Disable TensorBoard for simplicity\n",
    "        patience=3,  # Early-stopping patience\n",
    "        monitor=\"val_loss\",  # Monitor validation loss\n",
    "    )\n",
    "\n",
    "    history = compile_and_fit(  # Train the model using the shared utility\n",
    "        model,  # Model to train\n",
    "        train_prepared,  # Training dataset pipeline\n",
    "        optimizer=config[\"optimizer\"],  # Optimizer instance\n",
    "        loss=\"sparse_categorical_crossentropy\",  # Loss for classification\n",
    "        metrics=[\"accuracy\"],  # Track accuracy during training\n",
    "        epochs=config.get(\"epochs\", 5),  # Number of training epochs\n",
    "        validation_ds=val_prepared,  # Validation dataset pipeline\n",
    "        callbacks=callbacks,  # Training callbacks\n",
    "        verbose=2,  # Verbosity level\n",
    "    )\n",
    "\n",
    "    eval_results = model.evaluate(test_prepared, verbose=0)  # Evaluate the trained model on the test set\n",
    "    test_metrics = dict(zip(model.metrics_names, eval_results))  # Pair metric names with their computed values\n",
    "\n",
    "    return {  # Package the experiment artifacts\n",
    "        \"config\": config,  # Original configuration\n",
    "        \"history\": history,  # Training history\n",
    "        \"model\": model,  # Trained model\n",
    "        \"log_dir\": log_dir,  # Directory containing training logs\n",
    "        \"test_metrics\": test_metrics,  # Evaluation metrics on the test set\n",
    "        \"dataset_info\": info,  # Dataset metadata\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315063b3",
   "metadata": {},
   "source": [
    "## Fashion-MNIST: Optimizer Face-off\n",
    "\n",
    "We evaluate vanilla SGD, SGD with momentum, and Adam on the Fashion-MNIST dataset while keeping regularization fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_experiments = [  # Define experiments for the Fashion MNIST dataset\n",
    "    {  # Configuration using plain SGD\n",
    "        \"name\": \"fashion_sgd\",  # Unique experiment identifier\n",
    "        \"dataset\": \"fashion_mnist\",  # Dataset to load\n",
    "        \"optimizer\": tf.keras.optimizers.SGD(learning_rate=0.05),  # Optimizer setup\n",
    "        \"epochs\": 5,  # Number of training epochs\n",
    "        \"dropout_rate\": 0.3,  # Dropout for regularization\n",
    "        \"l2_factor\": 1e-4,  # L2 regularization strength\n",
    "        \"augment\": True,  # Enable data augmentation\n",
    "        \"batch_size\": 64,  # Mini-batch size\n",
    "        \"max_train_size\": 12000,  # Limit training set size\n",
    "        \"max_val_size\": 2000,  # Limit validation set size\n",
    "        \"max_test_size\": 4000,  # Limit test set size\n",
    "    },\n",
    "    {  # Configuration using SGD with momentum and Nesterov acceleration\n",
    "        \"name\": \"fashion_momentum\",  # Unique experiment identifier\n",
    "        \"dataset\": \"fashion_mnist\",  # Dataset to load\n",
    "        \"optimizer\": tf.keras.optimizers.SGD(learning_rate=0.03, momentum=0.9, nesterov=True),  # Optimizer setup\n",
    "        \"epochs\": 5,  # Number of training epochs\n",
    "        \"dropout_rate\": 0.3,  # Dropout for regularization\n",
    "        \"l2_factor\": 1e-4,  # L2 regularization strength\n",
    "        \"augment\": True,  # Enable data augmentation\n",
    "        \"batch_size\": 64,  # Mini-batch size\n",
    "        \"max_train_size\": 12000,  # Limit training set size\n",
    "        \"max_val_size\": 2000,  # Limit validation set size\n",
    "        \"max_test_size\": 4000,  # Limit test set size\n",
    "    },\n",
    "    {  # Configuration using Adam optimizer\n",
    "        \"name\": \"fashion_adam\",  # Unique experiment identifier\n",
    "        \"dataset\": \"fashion_mnist\",  # Dataset to load\n",
    "        \"optimizer\": tf.keras.optimizers.Adam(learning_rate=0.001),  # Optimizer setup\n",
    "        \"epochs\": 5,  # Number of training epochs\n",
    "        \"dropout_rate\": 0.3,  # Dropout for regularization\n",
    "        \"l2_factor\": 1e-4,  # L2 regularization strength\n",
    "        \"augment\": True,  # Enable data augmentation\n",
    "        \"batch_size\": 64,  # Mini-batch size\n",
    "        \"max_train_size\": 12000,  # Limit training set size\n",
    "        \"max_val_size\": 2000,  # Limit validation set size\n",
    "        \"max_test_size\": 4000,  # Limit test set size\n",
    "    },\n",
    "]\n",
    "\n",
    "fashion_results = [run_experiment(cfg) for cfg in fashion_experiments]  # Run the Fashion MNIST experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b292dc0",
   "metadata": {},
   "source": [
    "## MNIST: Regularization Ablations\n",
    "\n",
    "With Adam fixed as the optimizer, we vary dropout, weight decay, and augmentation to see how regularization influences generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b36a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_experiments = [  # Define experiments for the MNIST dataset\n",
    "    {  # Lightly regularized configuration\n",
    "        \"name\": \"mnist_min_reg\",  # Unique experiment identifier\n",
    "        \"dataset\": \"mnist\",  # Dataset to load\n",
    "        \"optimizer\": tf.keras.optimizers.Adam(learning_rate=0.001),  # Optimizer setup\n",
    "        \"epochs\": 4,  # Number of training epochs\n",
    "        \"dropout_rate\": 0.1,  # Dropout for regularization\n",
    "        \"l2_factor\": 0.0,  # No L2 regularization\n",
    "        \"augment\": False,  # Do not apply augmentation\n",
    "        \"batch_size\": 64,  # Mini-batch size\n",
    "        \"max_train_size\": 10000,  # Limit training set size\n",
    "        \"max_val_size\": 2000,  # Limit validation set size\n",
    "        \"max_test_size\": 4000,  # Limit test set size\n",
    "    },\n",
    "    {  # Heavier regularization with dropout and weight decay\n",
    "        \"name\": \"mnist_dropout_weightdecay\",  # Unique experiment identifier\n",
    "        \"dataset\": \"mnist\",  # Dataset to load\n",
    "        \"optimizer\": tf.keras.optimizers.Adam(learning_rate=0.001),  # Optimizer setup\n",
    "        \"epochs\": 4,  # Number of training epochs\n",
    "        \"dropout_rate\": 0.5,  # Dropout for regularization\n",
    "        \"l2_factor\": 1e-3,  # L2 regularization strength\n",
    "        \"augment\": True,  # Enable simple augmentation\n",
    "        \"batch_size\": 64,  # Mini-batch size\n",
    "        \"max_train_size\": 10000,  # Limit training set size\n",
    "        \"max_val_size\": 2000,  # Limit validation set size\n",
    "        \"max_test_size\": 4000,  # Limit test set size\n",
    "    },\n",
    "]\n",
    "\n",
    "mnist_results = [run_experiment(cfg) for cfg in mnist_experiments]  # Run the MNIST experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506c361",
   "metadata": {},
   "source": [
    "## Aggregate Metrics\n",
    "\n",
    "We summarize best validation performance, highlight the epoch where it occurred, and report held-out test accuracy using the new `summarize_history` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f537e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Data analysis library for summarizing results\n",
    "\n",
    "all_results = fashion_results + mnist_results  # Combine experiment outputs\n",
    "summary_rows: list[dict[str, Any]] = []  # Container for summary rows\n",
    "\n",
    "for result in all_results:  # Iterate over each experiment result\n",
    "    config = result[\"config\"]  # Retrieve the configuration dictionary\n",
    "    history_summary = summarize_history(result[\"history\"], metrics=[\"loss\", \"accuracy\"])  # Summarize training history\n",
    "    summary = {row[\"metric\"]: row for row in history_summary}  # Index summary rows by metric name\n",
    "\n",
    "    summary_rows.append(  # Collect summary information for comparison\n",
    "        {\n",
    "            \"experiment\": config[\"name\"],  # Experiment identifier\n",
    "            \"dataset\": config[\"dataset\"],  # Dataset name\n",
    "            \"optimizer\": type(config[\"optimizer\"]).__name__,  # Optimizer type\n",
    "            \"dropout\": config.get(\"dropout_rate\"),  # Dropout setting\n",
    "            \"l2_factor\": config.get(\"l2_factor\"),  # L2 regularization setting\n",
    "            \"augment\": config.get(\"augment\"),  # Augmentation flag\n",
    "            \"val_best_accuracy\": summary[\"accuracy\"].get(\"val_best\"),  # Best validation accuracy\n",
    "            \"val_accuracy_epoch\": summary[\"accuracy\"].get(\"val_epoch\"),  # Epoch of best validation accuracy\n",
    "            \"best_val_loss\": summary[\"loss\"].get(\"val_best\"),  # Best validation loss\n",
    "            \"test_accuracy\": result[\"test_metrics\"].get(\"accuracy\"),  # Test accuracy\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "comparison_df = pd.DataFrame(summary_rows)  # Build a DataFrame from the summaries\n",
    "display(comparison_df.sort_values(by=\"val_best_accuracy\", ascending=False).reset_index(drop=True))  # Display sorted results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb514729",
   "metadata": {},
   "source": [
    "## Visualize Training Dynamics\n",
    "\n",
    "Plotting the learning curves clarifies how quickly each configuration converges and whether it overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [plot_history(result[\"history\"], metrics=[\"accuracy\", \"loss\"]) for result in all_results]  # Plot accuracy and loss for each experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa159ae",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Adaptive optimizers such as Adam typically reach competitive validation accuracy faster on Fashion-MNIST, but SGD with momentum can close the gap with careful learning-rate tuning.\n",
    "- Stronger regularization (dropout + weight decay + augmentation) slows early training yet yields higher validation and test accuracy on MNIST, illustrating the bias\u2013variance trade-off.\n",
    "- Early stopping callbacks prevent divergence across all experiments, providing a safety net when hyperparameters are suboptimal.\n",
    "- The `summarize_history` helper offers a succinct way to extract the best-performing epochs for downstream reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Checks\n",
    "\n",
    "**Question 1.** How does adding momentum change the update direction compared with vanilla SGD?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Momentum forms a running average of recent gradients before applying the step.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "The update moves along $v_t$, a smoothed combination of past gradients, so directions that persist over multiple steps are amplified while oscillations cancel out.\n",
    "</details>\n",
    "\n",
    "**Question 2.** Why can $L_2$ weight decay improve generalization?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Think about how the penalty term changes the magnitude of the parameters.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "By shrinking weights toward zero, weight decay discourages overly large coefficients that fit noise, thereby reducing variance and improving performance on unseen data.\n",
    "</details>\n",
    "\n",
    "**Question 3.** What role does data augmentation play in this lab's regularization experiments?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Consider how augmented examples relate to the original dataset's support.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Augmentation broadens the training distribution with label-preserving transforms, making the model robust to small input perturbations and complementing other regularizers.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944826b9",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "1. **Optimizer Tuning Challenge:** Extend the optimizer sweep to include RMSProp and AdamW on Fashion-MNIST. Report the learning curves and discuss how decoupled weight decay in AdamW changes the results.\n",
    "2. **Regularization Grid Search:** For MNIST, run a 2\u00d72 grid over dropout rates (0.2, 0.4) and $L_2$ penalties ($1 \\times 10^{-4}$, $5 \\times 10^{-4}$). Summarize the outcomes in a table similar to the one above and reason about the best configuration.\n",
    "3. **Cross-Dataset Generalization:** Apply the full workflow to a third dataset of your choice (e.g., CIFAR-10 or EMNIST). Compare the effect of augmentation intensity on optimizer performance, highlighting any adjustments required in the model architecture.\n",
    "4. **Theory to Practice Essay:** In a short write-up (\u2248500 words), connect the empirical findings to the theoretical equations introduced at the start of the lab. Focus on how adaptive learning rates interact with regularization to shape the loss landscape traversal.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}