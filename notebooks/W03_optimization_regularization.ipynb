{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e144c8",
   "metadata": {},
   "source": [
    "# W03 · Optimization & Regularization\n",
    "\n",
    "This lab investigates how optimization algorithms and regularization strategies work together to improve deep learning models. You will combine theoretical intuition with practical TensorFlow experiments executed through the course utility helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d000cf",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab you will be able to:\n",
    "\n",
    "- Derive and interpret the update rules of popular first-order optimizers.\n",
    "- Explain how regularization mechanisms change the effective hypothesis space of neural networks.\n",
    "- Run disciplined experiments on multiple datasets (including **Fashion-MNIST**) using the course `dl_utils` helpers.\n",
    "- Compare and contrast the impact of optimization versus regularization choices on convergence speed and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b005c9",
   "metadata": {},
   "source": [
    "## Optimization Refresher\n",
    "\n",
    "The core of gradient-based learning is the iterative parameter update\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta_t),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $\\nabla_\\theta \\mathcal{L}(\\theta_t)$ is the gradient of the loss at iteration $t$. Several refinements build on this primitive:\n",
    "\n",
    "- **Momentum SGD** accumulates an exponential moving average of gradients, creating a velocity term $v_t$ for $g_t = \\nabla_\\theta \\mathcal{L}(\\theta_t)$:\n",
    "  $$\\begin{aligned}\n",
    "  v_t &= \\beta v_{t-1} + (1 - \\beta) g_t, \\\\n",
    "  \\theta_{t+1} &= \\theta_t - \\eta \\, v_t.\n",
    "  \\end{aligned}$$\n",
    "- **RMSProp** rescales the learning rate per-parameter using a running estimate of squared gradients $s_t$:\n",
    "  $$\\begin{aligned}\n",
    "  s_t &= \\beta s_{t-1} + (1 - \\beta) g_t^2, \\\\n",
    "  \\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\, g_t.\n",
    "  \\end{aligned}$$\n",
    "- **Adam** combines momentum and RMSProp with bias corrections:\n",
    "  $$\\begin{aligned}\n",
    "  m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t, \\\\n",
    "  v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2, \\\\n",
    "  \\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t}, \\\\n",
    "  \\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t}, \\\\n",
    "  \\theta_{t+1} &= \\theta_t - \\eta \\, \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}.\n",
    "  \\end{aligned}$$\n",
    "\n",
    "These optimizers differ in how aggressively they adapt the step size and in their sensitivity to poorly scaled gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c66fa6",
   "metadata": {},
   "source": [
    "## Regularization Refresher\n",
    "\n",
    "Regularization constrains the model to favor simpler or more robust solutions:\n",
    "\n",
    "- **$L_2$ weight decay** penalizes large weights by adding $\\frac{\\lambda}{2} \\lVert \\theta \\rVert_2^2$ to the loss, leading to the modified gradient\n",
    "  $$\\nabla_\\theta \\mathcal{L}_{\\text{reg}} = \\nabla_\\theta \\mathcal{L} + \\lambda \\theta.$$\n",
    "- **Dropout** randomly zeros activations with probability $p$ during training, sampling subnetworks that collectively prevent co-adaptation. The expected activation becomes $(1-p) h$ at inference time.\n",
    "- **Data augmentation** synthesizes additional training examples $\\tilde{x}$ sampled from transformations $T(x)$ that preserve labels, effectively enlarging the dataset support.\n",
    "- **Early stopping** halts training when validation loss stops improving, implicitly restricting the number of optimization steps and preventing overfitting.\n",
    "\n",
    "Each technique balances the bias–variance trade-off differently, shaping how well the model generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435bdc3",
   "metadata": {},
   "source": [
    "## Technique Comparison Cheat Sheet\n",
    "\n",
    "| Category | Technique | Primary Benefit | Key Hyperparameters | Typical Trade-offs |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Optimizer | SGD + Momentum | Smooths noisy gradients and accelerates along ravines | Learning rate $\\eta$, momentum $\\beta$ | Sensitive to $\\eta$; may stagnate on plateaus |\n",
    "| Optimizer | RMSProp | Adapts step sizes using squared-gradient averages | $\\eta$, decay $\\beta$, $\\epsilon$ | Can forget long-term gradient information |\n",
    "| Optimizer | Adam | Combines momentum and adaptivity for fast convergence | $\\eta$, $\\beta_1$, $\\beta_2$, $\\epsilon$ | May generalize worse than SGD on some tasks |\n",
    "| Regularization | $L_2$ Weight Decay | Shrinks weights to reduce variance | Penalty $\\lambda$ | Excessive decay underfits |\n",
    "| Regularization | Dropout | Prevents co-adaptation of units | Drop probability $p$ | Slows convergence; tuning $p$ is task-dependent |\n",
    "| Regularization | Data Augmentation | Expands dataset support to improve invariances | Transform family $T$, augmentation strength | Poorly chosen transforms can hurt accuracy |\n",
    "| Regularization | Early Stopping | Stops over-training based on validation metrics | Patience, monitored metric | Requires reliable validation signal |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98382eac",
   "metadata": {},
   "source": [
    "## Lab Roadmap\n",
    "\n",
    "1. Configure the environment and import the reusable helpers from `notebooks/dl_utils`.\n",
    "2. Build shared dataset pipelines for **Fashion-MNIST** and **MNIST**.\n",
    "3. Define a compact multilayer perceptron with configurable regularizers.\n",
    "4. Run optimization-focused experiments (SGD vs. Momentum vs. Adam) on Fashion-MNIST.\n",
    "5. Run regularization-focused experiments (dropout, weight decay, augmentation) on MNIST.\n",
    "6. Summarize and visualize the results, then reflect on the optimizer/regularizer interplay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b226e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from dl_utils import (\n",
    "    build_callbacks,\n",
    "    compile_and_fit,\n",
    "    load_tfds_dataset,\n",
    "    plot_history,\n",
    "    prepare_for_training,\n",
    "    summarize_history,\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8c842",
   "metadata": {},
   "source": [
    "### Dataset Preparation Helpers\n",
    "\n",
    "The helpers below rely on `dl_utils.load_tfds_dataset` and `dl_utils.prepare_for_training` to build consistent `tf.data` pipelines with optional augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940deedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetBundle:\n",
    "    name: str\n",
    "    train: tf.data.Dataset\n",
    "    validation: tf.data.Dataset\n",
    "    test: tf.data.Dataset\n",
    "    info: tfds.core.DatasetInfo\n",
    "\n",
    "\n",
    "def normalize_image(image: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    if image.shape.rank == 2:\n",
    "        image = tf.expand_dims(image, -1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_image_classification_data(\n",
    "    name: str,\n",
    "    *,\n",
    "    batch_size: int = 64,\n",
    "    validation_split: float = 0.1,\n",
    "    augment: bool = False,\n",
    "    max_train_size: int | None = 12000,\n",
    "    max_val_size: int | None = 2000,\n",
    "    max_test_size: int | None = 4000,\n",
    ") -> DatasetBundle:\n",
    "    \"\"\"Load an image classification dataset with normalized batches.\"\"\"\n",
    "\n",
    "    if not 0.0 < validation_split < 1.0:\n",
    "        raise ValueError(\"validation_split must be between 0 and 1\")\n",
    "\n",
    "    train_cut = int((1.0 - validation_split) * 100)\n",
    "    train_raw, info = load_tfds_dataset(name, split=f\"train[:{train_cut}%]\", with_info=True)\n",
    "    val_raw = load_tfds_dataset(name, split=f\"train[{train_cut}%:]\")\n",
    "    test_raw = load_tfds_dataset(name, split=\"test\")\n",
    "\n",
    "    if max_train_size is not None:\n",
    "        train_raw = train_raw.take(max_train_size)\n",
    "    if max_val_size is not None:\n",
    "        val_raw = val_raw.take(max_val_size)\n",
    "    if max_test_size is not None:\n",
    "        test_raw = test_raw.take(max_test_size)\n",
    "\n",
    "    def preprocess(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        return normalize_image(image), label\n",
    "\n",
    "    train_ds = train_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    augment_fn = None\n",
    "    if augment:\n",
    "        def augment_fn(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "            image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "            return image, label\n",
    "\n",
    "    train_ds = prepare_for_training(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        augment_fn=augment_fn,\n",
    "        shuffle_buffer=2048,\n",
    "    )\n",
    "    val_ds = prepare_for_training(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer=None,\n",
    "        cache=True,\n",
    "        prefetch=True,\n",
    "    )\n",
    "    test_ds = prepare_for_training(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer=None,\n",
    "        cache=True,\n",
    "        prefetch=True,\n",
    "    )\n",
    "\n",
    "    return DatasetBundle(name=name, train=train_ds, validation=val_ds, test=test_ds, info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14972a",
   "metadata": {},
   "source": [
    "### Model Builder\n",
    "\n",
    "We use a compact multilayer perceptron with tunable dropout and $L_2$ weight decay so that optimizer and regularization effects remain visible within a few epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_model(\n",
    "    input_shape: tuple[int, ...],\n",
    "    num_classes: int,\n",
    "    *,\n",
    "    dropout_rate: float = 0.3,\n",
    "    l2_factor: float = 1e-4,\n",
    ") -> tf.keras.Model:\n",
    "    regularizer = tf.keras.regularizers.l2(l2_factor) if l2_factor else None\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer),\n",
    "            tf.keras.layers.Dropout(dropout_rate / 2.0 if dropout_rate else 0.0),\n",
    "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda28bf",
   "metadata": {},
   "source": [
    "### Experiment Management\n",
    "\n",
    "The experiment runner stitches together dataset loading, model creation, compilation, and evaluation. Results are stored in a structured dictionary for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    @dataclass\n",
    "    class ExperimentConfig:\n",
    "        name: str\n",
    "        dataset: str\n",
    "        optimizer: tf.keras.optimizers.Optimizer\n",
    "        epochs: int = 5\n",
    "        batch_size: int = 64\n",
    "        dropout_rate: float = 0.3\n",
    "        l2_factor: float = 1e-4\n",
    "        augment: bool = False\n",
    "        max_train_size: int | None = 12000\n",
    "        max_val_size: int | None = 2000\n",
    "        max_test_size: int | None = 4000\n",
    "\n",
    "\n",
    "    def run_experiment(config: ExperimentConfig) -> dict[str, Any]:\n",
    "        print(f\"\n",
    "▶ Running experiment: {config.name}\")\n",
    "        data = load_image_classification_data(\n",
    "            config.dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            validation_split=0.1,\n",
    "            augment=config.augment,\n",
    "            max_train_size=config.max_train_size,\n",
    "            max_val_size=config.max_val_size,\n",
    "            max_test_size=config.max_test_size,\n",
    "        )\n",
    "\n",
    "        input_shape = data.info.features[\"image\"].shape\n",
    "        num_classes = data.info.features[\"label\"].num_classes\n",
    "\n",
    "        model = build_baseline_model(\n",
    "            input_shape,\n",
    "            num_classes,\n",
    "            dropout_rate=config.dropout_rate,\n",
    "            l2_factor=config.l2_factor,\n",
    "        )\n",
    "\n",
    "        callbacks, log_dir = build_callbacks(\n",
    "            experiment_name=config.name,\n",
    "            tensorboard=False,\n",
    "            patience=3,\n",
    "            monitor=\"val_loss\",\n",
    "        )\n",
    "\n",
    "        history = compile_and_fit(\n",
    "            model,\n",
    "            data.train,\n",
    "            optimizer=config.optimizer,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "            epochs=config.epochs,\n",
    "            validation_ds=data.validation,\n",
    "            callbacks=callbacks,\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "        eval_results = model.evaluate(data.test, verbose=0)\n",
    "        test_metrics = dict(zip(model.metrics_names, eval_results))\n",
    "\n",
    "        return {\n",
    "            \"config\": config,\n",
    "            \"history\": history,\n",
    "            \"model\": model,\n",
    "            \"log_dir\": log_dir,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"dataset_info\": data.info,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315063b3",
   "metadata": {},
   "source": [
    "## Fashion-MNIST: Optimizer Face-off\n",
    "\n",
    "We evaluate vanilla SGD, SGD with momentum, and Adam on the Fashion-MNIST dataset while keeping regularization fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_experiments = [\n",
    "    ExperimentConfig(\n",
    "        name=\"fashion_sgd\",\n",
    "        dataset=\"fashion_mnist\",\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "        epochs=5,\n",
    "        dropout_rate=0.3,\n",
    "        l2_factor=1e-4,\n",
    "        augment=True,\n",
    "        max_train_size=12000,\n",
    "        max_val_size=2000,\n",
    "    ),\n",
    "    ExperimentConfig(\n",
    "        name=\"fashion_momentum\",\n",
    "        dataset=\"fashion_mnist\",\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.03, momentum=0.9, nesterov=True),\n",
    "        epochs=5,\n",
    "        dropout_rate=0.3,\n",
    "        l2_factor=1e-4,\n",
    "        augment=True,\n",
    "        max_train_size=12000,\n",
    "        max_val_size=2000,\n",
    "    ),\n",
    "    ExperimentConfig(\n",
    "        name=\"fashion_adam\",\n",
    "        dataset=\"fashion_mnist\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        epochs=5,\n",
    "        dropout_rate=0.3,\n",
    "        l2_factor=1e-4,\n",
    "        augment=True,\n",
    "        max_train_size=12000,\n",
    "        max_val_size=2000,\n",
    "    ),\n",
    "]\n",
    "\n",
    "fashion_results = [run_experiment(cfg) for cfg in fashion_experiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b292dc0",
   "metadata": {},
   "source": [
    "## MNIST: Regularization Ablations\n",
    "\n",
    "With Adam fixed as the optimizer, we vary dropout, weight decay, and augmentation to see how regularization influences generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b36a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_experiments = [\n",
    "    ExperimentConfig(\n",
    "        name=\"mnist_min_reg\",\n",
    "        dataset=\"mnist\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        epochs=4,\n",
    "        dropout_rate=0.1,\n",
    "        l2_factor=0.0,\n",
    "        augment=False,\n",
    "        max_train_size=10000,\n",
    "        max_val_size=2000,\n",
    "    ),\n",
    "    ExperimentConfig(\n",
    "        name=\"mnist_dropout_weightdecay\",\n",
    "        dataset=\"mnist\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        epochs=4,\n",
    "        dropout_rate=0.5,\n",
    "        l2_factor=1e-3,\n",
    "        augment=True,\n",
    "        max_train_size=10000,\n",
    "        max_val_size=2000,\n",
    "    ),\n",
    "]\n",
    "\n",
    "mnist_results = [run_experiment(cfg) for cfg in mnist_experiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506c361",
   "metadata": {},
   "source": [
    "## Aggregate Metrics\n",
    "\n",
    "We summarize best validation performance, highlight the epoch where it occurred, and report held-out test accuracy using the new `summarize_history` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f537e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "all_results = fashion_results + mnist_results\n",
    "summary_rows: list[dict[str, Any]] = []\n",
    "\n",
    "for result in all_results:\n",
    "    config = result[\"config\"]\n",
    "    history_summary = summarize_history(result[\"history\"], metrics=[\"loss\", \"accuracy\"])\n",
    "    summary = {row[\"metric\"]: row for row in history_summary}\n",
    "\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"experiment\": config.name,\n",
    "            \"dataset\": config.dataset,\n",
    "            \"optimizer\": type(config.optimizer).__name__,\n",
    "            \"dropout\": config.dropout_rate,\n",
    "            \"l2_factor\": config.l2_factor,\n",
    "            \"augment\": config.augment,\n",
    "            \"val_best_accuracy\": summary[\"accuracy\"].get(\"val_best\"),\n",
    "            \"val_accuracy_epoch\": summary[\"accuracy\"].get(\"val_epoch\"),\n",
    "            \"best_val_loss\": summary[\"loss\"].get(\"val_best\"),\n",
    "            \"test_accuracy\": result[\"test_metrics\"].get(\"accuracy\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "comparison_df = pd.DataFrame(summary_rows)\n",
    "display(comparison_df.sort_values(by=\"val_best_accuracy\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb514729",
   "metadata": {},
   "source": [
    "## Visualize Training Dynamics\n",
    "\n",
    "Plotting the learning curves clarifies how quickly each configuration converges and whether it overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = [plot_history(result[\"history\"], metrics=[\"accuracy\", \"loss\"]) for result in all_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa159ae",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Adaptive optimizers such as Adam typically reach competitive validation accuracy faster on Fashion-MNIST, but SGD with momentum can close the gap with careful learning-rate tuning.\n",
    "- Stronger regularization (dropout + weight decay + augmentation) slows early training yet yields higher validation and test accuracy on MNIST, illustrating the bias–variance trade-off.\n",
    "- Early stopping callbacks prevent divergence across all experiments, providing a safety net when hyperparameters are suboptimal.\n",
    "- The `summarize_history` helper offers a succinct way to extract the best-performing epochs for downstream reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Checks\n",
    "\n",
    "**Question 1.** How does adding momentum change the update direction compared with vanilla SGD?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Momentum forms a running average of recent gradients before applying the step.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "The update moves along $v_t$, a smoothed combination of past gradients, so directions that persist over multiple steps are amplified while oscillations cancel out.\n",
    "</details>\n",
    "\n",
    "**Question 2.** Why can $L_2$ weight decay improve generalization?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Think about how the penalty term changes the magnitude of the parameters.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "By shrinking weights toward zero, weight decay discourages overly large coefficients that fit noise, thereby reducing variance and improving performance on unseen data.\n",
    "</details>\n",
    "\n",
    "**Question 3.** What role does data augmentation play in this lab's regularization experiments?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Consider how augmented examples relate to the original dataset's support.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Augmentation broadens the training distribution with label-preserving transforms, making the model robust to small input perturbations and complementing other regularizers.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944826b9",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "1. **Optimizer Tuning Challenge:** Extend the optimizer sweep to include RMSProp and AdamW on Fashion-MNIST. Report the learning curves and discuss how decoupled weight decay in AdamW changes the results.\n",
    "2. **Regularization Grid Search:** For MNIST, run a 2×2 grid over dropout rates (0.2, 0.4) and $L_2$ penalties ($1 \\times 10^{-4}$, $5 \\times 10^{-4}$). Summarize the outcomes in a table similar to the one above and reason about the best configuration.\n",
    "3. **Cross-Dataset Generalization:** Apply the full workflow to a third dataset of your choice (e.g., CIFAR-10 or EMNIST). Compare the effect of augmentation intensity on optimizer performance, highlighting any adjustments required in the model architecture.\n",
    "4. **Theory to Practice Essay:** In a short write-up (≈500 words), connect the empirical findings to the theoretical equations introduced at the start of the lab. Focus on how adaptive learning rates interact with regularization to shape the loss landscape traversal.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
